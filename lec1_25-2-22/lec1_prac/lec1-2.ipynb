{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imdb 다시 손코딩 해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 conda env 자동 활성화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.0 새로운 환경 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ conda create -n algolab\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 autoenv 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "참고 링크 : https://github.com/hyperupcall/autoenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ brew install 'autoenv'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 자동으로 실행될 명령어 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ echo \"conda activate alogolab\" >.env\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 해당 경로에서 터미널을 열면 자동으로 실행됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "CondaError: Run 'conda init' before 'conda activate'\n",
    "```\n",
    "위 에러가 나오지만 무시해도 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 시행착오1 direnv 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고 링크 : https://u2pia.medium.com/%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8-%EB%94%94%EB%A0%89%ED%86%A0%EB%A6%AC-%EB%93%A4%EC%96%B4%EA%B0%80%EB%A9%B4-%EC%9E%90%EB%8F%99%EC%9C%BC%EB%A1%9C-conda-%ED%99%98%EA%B2%BD-%EB%B6%88%EB%9F%AC%EC%98%A4%EA%B8%B0-direnv-7ed616ccea68"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 내용을 토대로 설치시 모든 경로에서 같은 env가 적용되는 문제가 있었음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5 시행착오2 WARNING 문구 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[WARNING]: Console output during zsh initialization detected.\n",
    "\n",
    "When using Powerlevel10k with instant prompt, console output during zsh\n",
    "initialization may indicate issues.\n",
    "\n",
    "You can:\n",
    "\n",
    "  - Recommended: Change ~/.zshrc so that it does not perform console I/O\n",
    "    after the instant prompt preamble. See the link below for details.\n",
    "\n",
    "    * You will not see this error message again.\n",
    "    * Zsh will start quickly and prompt will update smoothly.\n",
    "\n",
    "  - Suppress this warning either by running p10k configure or by manually\n",
    "    defining the following parameter:\n",
    "\n",
    "      typeset -g POWERLEVEL9K_INSTANT_PROMPT=quiet\n",
    "\n",
    "    * You will not see this error message again.\n",
    "    * Zsh will start quickly but prompt will jump down after initialization.\n",
    "\n",
    "  - Disable instant prompt either by running p10k configure or by manually\n",
    "    defining the following parameter:\n",
    "\n",
    "      typeset -g POWERLEVEL9K_INSTANT_PROMPT=off\n",
    "\n",
    "    * You will not see this error message again.\n",
    "    * Zsh will start slowly.\n",
    "\n",
    "  - Do nothing.\n",
    "\n",
    "    * You will see this error message every time you start zsh.\n",
    "    * Zsh will start quickly but prompt will jump down after initialization.\n",
    "\n",
    "For details, see:\n",
    "https://github.com/romkatv/powerlevel10k/blob/master/README.md#instant-prompt\n",
    "\n",
    "-- console output produced during zsh initialization follows --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 해결법 : .zshrc 파일에 아래 내용을 최하단에 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "typeset -g POWERLEVEL9K_INSTANT_PROMPT=quiet\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 requirements 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 파일 경로에 requirements.txt 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "transformers\n",
    "datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생성 후 위의 내용 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 requirements 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from -r requirements.txt (line 1)) (4.49.0)\n",
      "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 1)) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 1)) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 1)) (2.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 1)) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 1)) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 1)) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 1)) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 2)) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 2)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 2)) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 2)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 2)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->-r requirements.txt (line 2)) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets->-r requirements.txt (line 2)) (3.11.12)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers->-r requirements.txt (line 1)) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->datasets->-r requirements.txt (line 2)) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets->-r requirements.txt (line 2)) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 import 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 데이터셋 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('imdb')\n",
    "train_ds = ds['train']\n",
    "test_ds  = ds['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 checkpoint 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'distilbert-base-uncased'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|████████████████| 25000/25000 [00:03<00:00, 6520.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "def tokenize(texts):\n",
    "    return tokenizer(\n",
    "        texts['text'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select only the columns we need for training\n",
    "train_ds = train_ds.remove_columns(['text'])  # Remove the original text\n",
    "train_ds = train_ds.rename_column('label', 'labels')  # Rename 'label' to 'labels' as expected by the model\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_ds.set_format('torch')\n",
    "\n",
    "# Create DataLoader for batched training\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=8,  # Adjust based on your GPU memory\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor(0),\n",
       " 'input_ids': tensor([  101,  1045, 12524,  1045,  2572,  8025,  1011,  3756,  2013,  2026,\n",
       "          2678,  3573,  2138,  1997,  2035,  1996,  6704,  2008,  5129,  2009,\n",
       "          2043,  2009,  2001,  2034,  2207,  1999,  3476,  1012,  1045,  2036,\n",
       "          2657,  2008,  2012,  2034,  2009,  2001,  8243,  2011,  1057,  1012,\n",
       "          1055,  1012,  8205,  2065,  2009,  2412,  2699,  2000,  4607,  2023,\n",
       "          2406,  1010,  3568,  2108,  1037,  5470,  1997,  3152,  2641,  1000,\n",
       "          6801,  1000,  1045,  2428,  2018,  2000,  2156,  2023,  2005,  2870,\n",
       "          1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  1996,\n",
       "          5436,  2003,  8857,  2105,  1037,  2402,  4467,  3689,  3076,  2315,\n",
       "         14229,  2040,  4122,  2000,  4553,  2673,  2016,  2064,  2055,  2166,\n",
       "          1012,  1999,  3327,  2016,  4122,  2000,  3579,  2014,  3086,  2015,\n",
       "          2000,  2437,  2070,  4066,  1997,  4516,  2006,  2054,  1996,  2779,\n",
       "         25430, 14728,  2245,  2055,  3056,  2576,  3314,  2107,  2004,  1996,\n",
       "          5148,  2162,  1998,  2679,  3314,  1999,  1996,  2142,  2163,  1012,\n",
       "          1999,  2090,  4851,  8801,  1998,  6623,  7939,  4697,  3619,  1997,\n",
       "          8947,  2055,  2037, 10740,  2006,  4331,  1010,  2016,  2038,  3348,\n",
       "          2007,  2014,  3689,  3836,  1010, 19846,  1010,  1998,  2496,  2273,\n",
       "          1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,  1028,  2054,\n",
       "          8563,  2033,  2055,  1045,  2572,  8025,  1011,  3756,  2003,  2008,\n",
       "          2871,  2086,  3283,  1010,  2023,  2001,  2641, 26932,  1012,  2428,\n",
       "          1010,  1996,  3348,  1998, 16371, 25469,  5019,  2024,  2261,  1998,\n",
       "          2521,  2090,  1010,  2130,  2059,  2009,  1005,  1055,  2025,  2915,\n",
       "          2066,  2070, 10036,  2135,  2081, 22555,  2080,  1012,  2096,  2026,\n",
       "          2406,  3549,  2568,  2424,  2009, 16880,  1010,  1999,  4507,  3348,\n",
       "          1998, 16371, 25469,  2024,  1037,  2350, 18785,  1999,  4467,  5988,\n",
       "          1012,  2130, 13749,  7849, 24544,  1010, 15835,  2037,  3437,  2000,\n",
       "          2204,  2214,  2879,  2198,  4811,  1010,  2018,  3348,  5019,  1999,\n",
       "          2010,  3152,  1012,  1026,  7987,  1013,  1028,  1026,  7987,  1013,\n",
       "          1028,  1045,  2079,  4012,  3549,  2094,  1996, 16587,  2005,  1996,\n",
       "          2755,  2008,  2151,  3348,  3491,  1999,  1996,  2143,  2003,  3491,\n",
       "          2005,  6018,  5682,  2738,  2084,  2074,  2000,  5213,  2111,  1998,\n",
       "          2191,  2769,  2000,  2022,  3491,  1999, 26932, 12370,  1999,  2637,\n",
       "          1012,  1045,  2572,  8025,  1011,  3756,  2003,  1037,  2204,  2143,\n",
       "          2005,  3087,  5782,  2000,  2817,  1996,  6240,  1998, 14629,  1006,\n",
       "          2053, 26136,  3832,  1007,  1997,  4467,  5988,  1012,  2021,  2428,\n",
       "          1010,  2023,  2143,  2987,  1005,  1056,  2031,  2172,  1997,  1037,\n",
       "          5436,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|█████████████████████████| 100/100 [00:56<00:00,  1.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "Train Loss: 0.0000 | Acc: 100.00%\n",
      "Val Acc: 100.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|█████████████████████████| 100/100 [00:56<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "Train Loss: 0.0000 | Acc: 100.00%\n",
      "Val Acc: 100.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|█████████████████████████| 100/100 [00:56<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "Train Loss: 0.0000 | Acc: 100.00%\n",
      "Val Acc: 100.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋을 1000개 선택 (50 -> 1000으로 증가)\n",
    "small_train_ds = train_ds.select(range(1000))\n",
    "\n",
    "# 검증용 데이터셋 200개 별도 분리\n",
    "train_dataset = small_train_ds.select(range(800))\n",
    "val_dataset = small_train_ds.select(range(800, 1000))\n",
    "\n",
    "# 배치 사이즈 증가 (2 -> 8)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "# 학습률 조정 (5e-5 -> 1e-5)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# 훈련 루프 개선\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_samples = 0, 0, 0\n",
    "    \n",
    "    # tqdm으로 진행률 표시\n",
    "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # 정확도 계산\n",
    "        preds = torch.argmax(outputs.logits, dim=-1)\n",
    "        correct = (preds == batch['labels']).sum().item()\n",
    "        total_correct += correct\n",
    "        total_samples += batch['labels'].size(0)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # 검증 단계 추가\n",
    "    model.eval()\n",
    "    val_correct, val_total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            val_correct += (preds == batch['labels']).sum().item()\n",
    "            val_total += batch['labels'].size(0)\n",
    "    \n",
    "    # 결과 출력\n",
    "    train_acc = (total_correct / total_samples) * 100\n",
    "    val_acc = (val_correct / val_total) * 100\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}:\")\n",
    "    print(f\"Train Loss: {avg_loss:.4f} | Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Acc: {val_acc:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
